{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0783b950-8ca0-4011-bd1b-08a05022aa5d",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# Table of Contents\n",
    "\n",
    "1. [Project Goal & Dataset](#pgd)\n",
    "2. [PySpark Installation](#pysi)\n",
    "3. [Data Analysis](#da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959575d5-b69c-4a84-b051-64365e2a0419",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id='pgd'></a>\n",
    "# Project Goal\n",
    "\n",
    "Showcase how pyspark can be used for answering analytics questions for a dataset from Uber case Study.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/nanasahebshinde/uber-case-study\n",
    "\n",
    "### Data Dictionary:\n",
    "\n",
    "| Column  | Definition |\n",
    "| --- | --- |\n",
    "| Date |  |\n",
    "| Time(Local) | Hour of the day |\n",
    "| App Openings (Eyeballs) | Customers who launch the app looking for riders. It is a good measure of demand |\n",
    "| Zeroes | Customers who open the app and see no cars in the area. |\n",
    "| Requests | Customers who make requests for a car. |\n",
    "| Completed Trip | The point from when a customer is picked |\n",
    "| Unique Drivers | Drivers who logged in |\n",
    "\n",
    "### Questions we would answer\n",
    "\n",
    "1.  Which date had the most completed trips during the two-week period?\n",
    "2.  What was the highest number of completed trips within a 24-hour period?\n",
    "3.  Which hour of the day had the most requests during the two-week period?\n",
    "4.  What percentages of all zeroes during the two-week period occurred on weekend (Friday at 5 pm to Sunday at 3 am)? Tip: The local time value is the start of the hour (e.g. `15` is the hour from 3:00 pm - 4:00 pm)\n",
    "5.  What is the weighted average ratio of completed trips per driver during the two-week period? Tip: \"Weighted average\" means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "6.  In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "7.  True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed.\n",
    "8.  In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "9.  If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing\n",
    "10. Looking at the data from all two weeks, which time might make the most sense to consider a true \"end day\" instead of midnight? (i.e when are supply and demand at both their natural minimums)\n",
    "\n",
    "[back to top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc118db-dedc-44b2-a111-b9dab56a4153",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id='pysi'></a>\n",
    "# PySpark Installation \\[Windows\\]\n",
    "\n",
    "1. Download and Install python. Only python 3.10 seems to be working with latest spark. Download Link: https://www.python.org/downloads/release/python-3100/\n",
    "2. Dowload and install JDK8 installer from the Oracle website. Download Link: https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html\n",
    "Add **JAVA_HOME** to your environment variables and set it to downloaded extracted jdk.\n",
    "5. Download latest Apache Spark from:  https://spark.apache.org/downloads.html. Set SPARK_HOME in environment variables where spark is extracted.\n",
    "6. Setup Hadoop\n",
    "   1. create a folder with following structure anywhere: **hadoop/bin**\n",
    "   2. Download and extract only **winutils.exe** from here: https://github.com/steveloughran/winutils  to this **hadoop/bin** folder. Latest version works. Download link: https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin\n",
    "   3. set HADOOP_HOME environment variable to this **path_where_u_create this folder/hadoop** directory.\n",
    "7. Install pyspark: pip install pyspark\n",
    "8. Test if pyspark works.\n",
    "\n",
    "   The following code should work\n",
    "\n",
    "   `>>> from pyspark.sql import SparkSession`\n",
    "   \n",
    "    `>>> spark = SparkSession.builder.master(\"local\").appName(\"PySpark Installation Test\").getOrCreate()`\n",
    "\n",
    "    `>>> df = spark.createDataFrame([(1, \"Hello\"), (2, \"World\")], [\"id\", \"message\"])`\n",
    "\n",
    "    `>>> df.show()`\n",
    "\n",
    "\n",
    "[back to top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d2a05-18da-4d69-af5a-cca418c1e862",
   "metadata": {},
   "source": [
    "<a id='da'></a>\n",
    "# Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12946037-1e14-41ce-a30f-7aff72b09f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "|     Date|Time (Local)|Eyeballs|Zeroes|Completed Trips|Requests|Unique Drivers|\n",
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "|10-Sep-12|           7|       5|     0|              2|       2|             9|\n",
      "|10-Sep-12|           8|       6|     0|              2|       2|            14|\n",
      "|10-Sep-12|           9|       8|     3|              0|       0|            14|\n",
      "|10-Sep-12|          10|       9|     2|              0|       1|            14|\n",
      "|10-Sep-12|          11|      11|     1|              4|       4|            11|\n",
      "+---------+------------+--------+------+---------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"PySpark Installation Test\").getOrCreate()\n",
    "\n",
    "# Read the data from CSV file\n",
    "uber = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True)\n",
    "uber.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6808ad8-4e49-414c-963c-f5da8cd92e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-Sep-12\n"
     ]
    }
   ],
   "source": [
    "# 1. Which date had the most completed trips during the two-week period?\n",
    "# Group the data by date and sum the completed trips\n",
    "completed_trips_by_date = uber.groupBy(\"Date\").sum(\"Completed Trips\")\n",
    "\n",
    "# Find the date with the most completed trips\n",
    "date_with_most_completed_trips = completed_trips_by_date \\\n",
    "    .orderBy(\"sum(Completed Trips)\", ascending=False) \\\n",
    "    .select(\"Date\") \\\n",
    "    .first()[\"Date\"]\n",
    "\n",
    "print(date_with_most_completed_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc12562-dbda-428c-87e6-4a6943681f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. What was the highest number of completed trips within a 24-hour period?\n",
    "# Question is not clear one: it could be the max no of trips evry day, which accounts for 24 hours or it could\n",
    "# be a window of 24 hour starting from current hour. I will use the second one.\n",
    "# Since there are no missing hours on any day, the simple window selecting 24 rows would be fine\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum\n",
    "w = Window.rowsBetween(0, 23)\n",
    "uber.withColumn('ra', sum('Completed Trips').over(w)).orderBy(\"ra\", ascending=False).first()['ra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8648d83a-546d-4d47-bc49-7b8defde15d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 .Which hour of the day had the most requests during the two-week period?\n",
    "from pyspark.sql.functions import sum\n",
    "uber.groupBy(\"Time (Local)\").sum(\"Requests\").orderBy(\"sum(Requests)\", ascending=False).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cb630e-cd5c-4a41-a824-9f35ca1f132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of zeros that occurred on weekends is: 29.111266620014\n"
     ]
    }
   ],
   "source": [
    "#4. What percentages of all zeroes during the two-week period occurred on weekend \n",
    "# (Friday at 5 pm to Sunday at 3 am)? Tip: The local time value is the start of the \n",
    "# hour (e.g. 15 is the hour from 3:00 pm - 4:00 pm)\n",
    "# dayofweek is 1 for Sunday\n",
    "from pyspark.sql.functions import dayofweek,to_timestamp\n",
    "\n",
    "uwdw = uber.withColumn('dw',dayofweek(to_timestamp(uber['Date'],\"dd-MMM-yy\")))\n",
    "weekend_zeros  = uwdw.filter(((uwdw['dw'] == 6)|(uwdw['dw'] == 7)) & ((uber[\"Time (Local)\"] >= 17)| (uber[\"Time (Local)\"] <3)))\\\n",
    ".agg(sum(\"Zeroes\")).collect()[0]['sum(Zeroes)']\n",
    "total_zeros= uber.agg(sum(\"Zeroes\").alias(\"total_zeros\")).collect()[0][\"total_zeros\"]\n",
    "print(\"The percentage of zeros that occurred on weekends is:\",weekend_zeros / total_zeros * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50609e52-6106-444a-b73e-4265b1a0b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted average ratio of completed trips per driver is: 0.8276707747535554\n"
     ]
    }
   ],
   "source": [
    "#5. What is the weighted average ratio of completed trips per driver during the two-week period? \n",
    "# Tip: “Weighted average” means your answer should account for the total trip volume in each hour \n",
    "# to determine the most accurate number in the whole period.\n",
    "from pyspark.sql.functions import avg,col\n",
    "\n",
    "weighted_avg = uber.withColumn(\"completed_per_driver\", uber[\"Completed Trips\"] / uber[\"Unique Drivers\"]) \\\n",
    "                 .groupBy(\"Date\", \"Time (Local)\") \\\n",
    "                 .agg(avg(\"completed_per_driver\").alias(\"avg_completed_per_driver\"), sum(\"Completed Trips\").alias(\"total_completed_trips\")) \\\n",
    "                 .withColumn(\"weighted_ratio\", col(\"avg_completed_per_driver\") * col(\"total_completed_trips\")) \\\n",
    "                 .agg(sum(\"weighted_ratio\") / sum(\"total_completed_trips\")).collect()[0][0]\n",
    "\n",
    "print(\"The weighted average ratio of completed trips per driver is:\", weighted_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0358b57-9f63-4278-ade4-bb61449d5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------------+\n",
      "|Time (Local)|unique_requests|consecutive_sum|\n",
      "+------------+---------------+---------------+\n",
      "|          20|             12|             80|\n",
      "+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours \n",
    "#over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that \n",
    "#a driver will work the same shift each day.\n",
    "from pyspark.sql.functions import col, hour, countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate the number of unique requests for each hour of the day\n",
    "hourly_unique_requests = (uber\n",
    "  .groupBy(\"Time (Local)\")).agg(countDistinct(\"Requests\").alias(\"unique_requests\"))\n",
    "\n",
    "# Slide a window of 8 hours to find the busiest 8 consecutive hours\n",
    "window = Window.orderBy(col(\"unique_requests\").desc()).rowsBetween(0, 7)\n",
    "busiest_8_consecutive_hours = (hourly_unique_requests\n",
    "  .select(\"*\", sum(\"unique_requests\").over(window).alias(\"consecutive_sum\"))\n",
    "  .orderBy(col(\"consecutive_sum\").desc())\n",
    "  .limit(1)\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "busiest_8_consecutive_hours.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70556a2-dc12-4f3f-8073-5983087e5226",
   "metadata": {},
   "source": [
    "7. True or False: Driver supply always increases when demand increases during the two-week period.\n",
    "\n",
    "This statement is false. There are multiple reasons why driver supply might not always increase when demand increases. For example, some drivers might choose not to work during peak demand times, or there might be external factors that affect driver availability (such as traffic, weather conditions, or events in the city). To confirm this, we would need to analyze the data and identify instances where demand increased but driver supply did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6dff8c-5df7-4e6b-a919-24cebff595f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------+-------------------+\n",
      "|period|zeroes|eyeballs|              ratio|\n",
      "+------+------+--------+-------------------+\n",
      "|  5199|   434|    1783|0.24340998317442514|\n",
      "+------+------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
    "from pyspark.sql.functions import col, sum, concat_ws\n",
    "\n",
    "uber = uber.withColumn('date_hour', concat_ws(' ', uber.Date,uber['Time (Local)']))\n",
    "# Group the data by 72-hour periods and calculate the ratio of zeroes to eyeballs for each period\n",
    "period_ratios = (uber.groupBy(((to_timestamp(uber['date_hour'],\"dd-MMM-yy H\").cast(\"long\")/ (72*3600)).cast(\"int\")).alias(\"period\"))\\\n",
    ".agg(sum(\"Zeroes\").alias(\"zeroes\"), sum(\"Eyeballs\").alias(\"eyeballs\"))\n",
    ".withColumn(\"ratio\", col(\"zeroes\") / col(\"eyeballs\"))\n",
    ")\n",
    "\n",
    "# Find the period with the highest ratio\n",
    "highest_ratio_period = period_ratios.orderBy(col(\"ratio\").desc()).limit(1)\n",
    "\n",
    "# Print the result\n",
    "highest_ratio_period.show() #2-hour period is the ratio of Zeroes to Eyeballs the highest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21522da8-4143-4f99-8fc0-0055061cc6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|Time (Local)|requests_per_driver|\n",
      "+------------+-------------------+\n",
      "|           2|               20.0|\n",
      "+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9.If you could add 5 drivers to any single hour of every day during the two-week period, which hour \n",
    "#should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing.\n",
    "# Calculate requests per unique driver for each hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "requests_per_driver = (uber.groupBy('Time (Local)').agg(\n",
    "    (F.sum('Requests') / F.countDistinct('Unique Drivers')).alias('requests_per_driver'))\n",
    ")\n",
    "\n",
    "# Show the hour with the highest ratio\n",
    "requests_per_driver.orderBy(F.desc('requests_per_driver')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852a3a94-df34-462c-af1c-eec8af5c8919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------------+\n",
      "|Time (Local)|avg_completed_trips|avg_unique_drivers|\n",
      "+------------+-------------------+------------------+\n",
      "|           4|0.14285714285714285|0.6428571428571429|\n",
      "+------------+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10.Looking at the data from all two weeks, which time might make the most sense to consider a true \n",
    "# “end day” instead of midnight? (i.e when are supply and demand at both their natural minimums)\n",
    "# Calculate average completed trips and unique drivers for each hour\n",
    "avg_trips_and_drivers = (uber.groupBy('Time (Local)').agg(\n",
    "    F.mean('Completed Trips').alias('avg_completed_trips'),\n",
    "    F.mean('Unique Drivers').alias('avg_unique_drivers')\n",
    "))\n",
    "\n",
    "# Show the hour with the lowest average completed trips and unique drivers\n",
    "avg_trips_and_drivers.orderBy('avg_completed_trips', 'avg_unique_drivers').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb06ec7-e706-4672-8015-37678f726064",
   "metadata": {},
   "source": [
    "[back to top](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6869cae-94f9-49df-aa53-c2e67f362d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
